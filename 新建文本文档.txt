ReLU之类的激活函数存在的意义是：
如果不进行激活，
那么输入数据的所有线性变换都可以写成
A*Q(Q=B*C*D*...)的形式。
只要一个再乘一个Q^-1就会变成A的形式。
换句话说，网络只是对A做了一系列的线性变换。
这意味着，这个网络只能做线性的输出，

也就是只能画直线，即便这是一个高维的直线。

那么，引入一个非线性的映射，就使得输出更逼近曲线，是一种提升网络表达能力的方法。
并且这种非线性的，且单增的映射，使得网络可以根据对输出的验证，通过链式求导的方法反向追踪到输入值，并且判断出该值对输出的影响，并根据影响的大小进行权值更新。



tanh和sigmoid（很像高中生物的S型曲线（英文就叫：sigmoid growth curve））
为代表的饱和函数，会在输入过大或者过小的时候趋近于0。
（sigmoid的导数甚至全小于1/4）
又由于追踪的时候是用的链式相乘，于是追溯到的影响就很小，这就使得权值几乎不更新。造成梯度消失问题。

或许可以把数据归一化来尽量避免这样的问题？

还有一点是：
由于权值的更新依赖于激活函数，
而sigmoid的输出全为正值，无法体现正负值的差异，这就造成权值在逼近最优解时只能走连续的之字路线，不能走折现曲线，优化效率十分低下。


ReLU为代表的非饱和函数，导数为0或者1，不存在梯度消失。但是因为ReLU的导数在输入小于0时直接为0，这就导致ReLU函数很容易造成神经元死亡，即权值更新为0

虽然ReLU是一个分段线性函数，
但是他能够获得非线性的结果。

因为在他的选择下，每次权值更新后的W矩阵是不等价的。即对W的更新是非线性的，这导致了一个非线性的分类结果。

为了解决ReLU死亡的问题，
Leaky ReLU 将 负值X 的输出定义为了一个小斜率的值aX，通常a为0.01到1e-6左右。


有待商榷的地方：
1、激活函数怎么取得一个非线性的结果的？
2、数据归一化可以解决梯度消失问题吗？
如果是在图像识别领域，所有值都是0~255的情况下呢？
3、分段线性函数ReLU实现非线性输出的具体方法？


